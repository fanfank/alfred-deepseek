#!/usr/bin/osascript -l JavaScript

const AssistantName = "deepseek"

// indicate which model to use
const Mode = {
  // Mode constants
  normalMode: "0",
  powerfulMode: "1",
  botMode: "2",

  // Private variable, using closure protection
  _mode: "0",

  // Get current mode
  get() {
    return this._mode
  },
  
  // Set internal state only
  set(value) {
    if (typeof value !== 'string') {
      value = String(value)
    }
    this._mode = value
    return this._mode
  },

  // Initialize from environment variable
  initFromEnv() {
    const isBotMode = envVar("bot_mode")
    const isPowerfulMode = envVar("powerful_mode")
    if (isBotMode === "1") {
      this._mode = this.botMode
    } else if (isPowerfulMode === "1") {
      this._mode = this.powerfulMode
    } else {
      this._mode = this.normalMode
    }
    return this._mode
  },

  isBotMode() {
    return this._mode === this.botMode
  },

  isPowerfulMode() {
    return this._mode === this.powerfulMode
  },
}

// Define a Provider class to manage the provider state
const Provider = {
  // Provider constants
  volcengineProvider: "volcengine",
  deepseekProvider: "deepseek",
  openrouterProvider: "openrouter",

  // Private variable, using closure protection
  _provider: "",

  // Get current provider
  get() {
    return this._provider
  },
  
  // Set internal state only
  set(value) {
    if (typeof value !== 'string') {
      value = String(value)
    }
    this._provider = value
    return this._provider
  },

  // Initialize from environment variable
  initFromEnv() {
    this._provider = envVar("provider")
    return this._provider
  },

  isVolcengine() {
    return this._provider === this.volcengineProvider
  },

  isDeepseek() {
    return this._provider === this.deepseekProvider
  },

  isOpenrouter() {
    return this._provider === this.openrouterProvider
  },
}

// Define a TimeoutSeconds class to manage timeout settings in seconds
const TimeoutSeconds = {
  // Default timeout in seconds
  defaultValue: 10,

  // Private variable for timeout value
  _value: 10,

  // Get current timeout value
  get() {
    return this._value
  },

  // Initialize from environment variable
  initFromEnv() {
    this._value = parseInt(envVar("timeout_seconds") || this.defaultValue.toString())
    return this._value
  },

  // Check if a timestamp is stalled
  isStalled(timestamp) {
    return new Date().getTime() - timestamp > this._value * 1000
  }
}

function envVar(varName) {
  return $.NSProcessInfo
    .processInfo
    .environment
    .objectForKey(varName).js
}

function fileExists(path) {
  return $.NSFileManager.defaultManager.fileExistsAtPath(path)
}

function fileModified(path) {
  return $.NSFileManager.defaultManager
    .attributesOfItemAtPathError(path, undefined)
    .js["NSFileModificationDate"].js
    .getTime()
}

function writeFile(path, text) {
  $(text).writeToFileAtomicallyEncodingError(path, true, $.NSUTF8StringEncoding, undefined)
}

function readFile(path) {
  return $.NSString.stringWithContentsOfFileEncodingError(path, $.NSUTF8StringEncoding, undefined).js
}

function deleteFile(path) {
  return $.NSFileManager.defaultManager.removeItemAtPathError(path, undefined)
}

function debugLogToFile(...args) {
  const debugLogFile = `${envVar("alfred_workflow_cache")}/${AssistantName}_debug.txt`
  const text = args.map(arg => {
    if (typeof arg === 'object') {
      try {
        return JSON.stringify(arg, null, 2)
      } catch {
        return String(arg)
      }
    }
    return String(arg)
  }).join(' ')
  writeFile(debugLogFile, text)
}

function killProcess(pid) {
  const task = $.NSTask.alloc.init
  task.executableURL = $.NSURL.fileURLWithPath("/bin/kill")
  task.arguments = [pid.toString()]
  task.launchAndReturnError(false)
}

function forcedCleanup(pidStreamFile, streamFile) {
  const pid = readFile(pidStreamFile).toString().trim()
  if (pid) killProcess(pid)

  deleteFile(pidStreamFile)
  deleteFile(streamFile)
}

function readChat(path) {
  const chatString = $.NSString.stringWithContentsOfFileEncodingError(path, $.NSUTF8StringEncoding, undefined).js
  return JSON.parse(chatString)
}

function appendChat(path, message) {
  const ongoingChat = readChat(path).concat(message)
  const chatString = JSON.stringify(ongoingChat)
  writeFile(path, chatString)
}

function appendAssistantChat(path, reasoningText, contentText) {
  if (!reasoningText && !contentText) return
  const message = {
    role: "assistant",
    reasoning_content: reasoningText ?? "",
    content: contentText ?? ""
  }
  appendChat(path, message)
}

// extract reasoning content, content and finish reason from streamString
// if not available, empty string will be set to the corresponding variable
function extractContentFromStreamString(streamString) {
  if (streamString.startsWith("{")) return { reasoningText: "", contentText: "", finishReason: "" }

  let finishReason = ""

  const chunks = streamString
    .split("\n") // Split into lines
    .filter(item => item && !item.startsWith(": OPENROUTER PROCESSING")) // Remove empty lines and lines starting with ": OPEN..."
    .map(item => item.replace(/^data: ?/, "")) // Remove extraneous "data: "
    .flatMap(item => { 
      try { 
        const res = JSON.parse(item)
        if (typeof res.choices?.[0]?.finish_reason === 'string' && res.choices[0].finish_reason.length > 0) {
          finishReason = res.choices[0].finish_reason
        }

        return res
      } catch { return [] }}) // Parse as JSON

  // detect and extract reasoning content
  let hasReasoningContent = false
  const reasoningText = chunks
    .map(item => {
      let reasoningContent = ""
      const choice = item["choices"]?.[0]
      const rawReasoningContent = (choice?.["delta"]?.["reasoning_content"] ?? choice?.["delta"]?.["reasoning"] ?? "")
      
      if (hasReasoningContent) {
        reasoningContent = rawReasoningContent.replaceAll("\n", "\n> ")
      } else if (rawReasoningContent.length > 0) {
        hasReasoningContent = true
        reasoningContent = rawReasoningContent
          .replace(/^/, "> ")
          .replaceAll("\n", "\n> ")
      }
      return reasoningContent
    })
    .join("")
    .concat(hasReasoningContent ? "\n\n" : "")

  // extract content
  const contentText = chunks
    .map(item => {
      const choice = item["choices"]?.[0]
      const content = choice?.["delta"]["content"] ?? ""
      return content
    })
    .join("")

  return {
    reasoningText,
    contentText,
    finishReason
  }
}

function markdownChat(messages, ignoreLastInterrupted = true) {
  return messages.reduce((accumulator, current, index, allMessages) => {
    if (current["role"] === "assistant") {
      return `${accumulator}${current["reasoning_content"]}${current["content"]}\n\n`
    }

    if (current["role"] === "user") {
      const currentMode = current?.["mode"] ?? current?.["powerful_mode"]
      const userRawMessage = current["content"].split("\n").map(line => `${line}`).join("\n") // support multi-line questions (e.g. via External Trigger)
      const messageIcon = currentMode === Mode.botMode ? "🌐" : currentMode == Mode.powerfulMode ? "💥" : "💬";
      const userMessage = `> ## ${messageIcon}：${userRawMessage}`
        .replace(/\n(?!$)/g, "\n>##"); // add style to user message


      const userTwice = allMessages[index + 1]?.["role"] === "user" // "user" role twice in a row
      const lastMessage = index === allMessages.length - 1 // "user" is last message

      return userTwice || (lastMessage && !ignoreLastInterrupted) ?
        `${accumulator}${userMessage}\n\n[Answer Interrupted]\n\n` :
        `${accumulator}${userMessage}\n\n`
    }

    // Ignore any other role
    return accumulator
  }, "")
}

function startStream(apiEndpoint, apiKey, targetModelEndpoint, systemPrompt, contextChat, streamFile, pidStreamFile) {
  $.NSFileManager.defaultManager.createFileAtPathContentsAttributes(streamFile, undefined, undefined) // Create empty file

  // remove reasoning_content from contextChat and build messages
  let messages = (systemPrompt ? [{ role: "system", content: systemPrompt }] : [])
  let roles = ["user", "assistant"]
  let roleIndex = 0
  
  // be compatible with the deepseek api
  for (const [index, item] of contextChat.entries()) {
    let expectRole = roles[roleIndex % 2]
    let message = { role: item.role, content: item.content } // only preserve role and content
    
    if (item.role !== expectRole) {
      if (expectRole === "user") { // user message is missing, skip this item
        continue;
      } else { // assistant message is missing, add an assistant message
        messages.push({ role: "assistant", content: "" }, message);
        continue;
      }
    }
    
    messages.push(message);
    roleIndex++;
  }

  const task = $.NSTask.alloc.init
  const stdout = $.NSPipe.pipe

  task.executableURL = $.NSURL.fileURLWithPath("/usr/bin/curl")
  task.arguments = [
    apiEndpoint,
    "--speed-limit", "0", "--speed-time", TimeoutSeconds.get().toString(), // Abort stalled connection after timeout
    "--silent", "--no-buffer",
    "--header", "Content-Type: application/json",
    "--header", `Authorization: Bearer ${apiKey}`,
    "--data", JSON.stringify({ model: targetModelEndpoint, messages: messages, stream: true }),
    "--output", streamFile
  ]

  task.standardOutput = stdout
  task.launchAndReturnError(false)
  writeFile(pidStreamFile, task.processIdentifier.toString())
}

function readStream(streamFile, chatFile, pidStreamFile) {
  const streamMarker = envVar("stream_marker") === "1"

  // When starting a stream or continuing from a closed window, add a marker to determine the location of future replacements
  if (streamMarker) return JSON.stringify({
    rerun: 0.1,
    variables: { streaming_now: true },
    response: "…",
    behaviour: { response: "append" }
  })

  // read HTTP response from streamFile
  const streamString = $.NSString.stringWithContentsOfFileEncodingError(streamFile, $.NSUTF8StringEncoding, undefined).js

  // If response looks like proper JSON, it is probably an error
  if (streamString.startsWith("{")) {
    try {
      const errorMessage = JSON.parse(streamString)["error"]["message"]

      if (errorMessage) {
        // Delete stream files
        deleteFile(streamFile)
        deleteFile(pidStreamFile)

        return JSON.stringify({
          actionoutput: Mode.isPowerfulMode() || Mode.isBotMode(),
          response: `[${errorMessage}]`, // Surround in square brackets to look like other errors
          behaviour: { response: "replacelast" }
        })
      }

      throw "Could not determine error message" // Fallback to the catch
    } catch {
      // If it's not an error from the API, log file contents
      return JSON.stringify({
        actionoutput: Mode.isPowerfulMode() || Mode.isBotMode(),
        response: streamString,
        behaviour: { response: "replacelast" }
      })
    }
  }

  const { reasoningText, contentText, finishReason } = extractContentFromStreamString(streamString)

  // response to user
  const responseText = reasoningText.concat(contentText)

  // If File not modified for over timeout seconds, connection stalled
  const stalled = TimeoutSeconds.isStalled(fileModified(streamFile))

  if (stalled) {
    // Write incomplete response
    appendAssistantChat(chatFile, reasoningText, contentText)

    // Delete stream files
    deleteFile(streamFile)
    deleteFile(pidStreamFile)

    // Stop
    return JSON.stringify({
      actionoutput: Mode.isPowerfulMode() || Mode.isBotMode(),
      response: `${responseText} [Connection Stalled]`,
      footer: `You can ask ${AssistantName} to continue the answer`,
      behaviour: { response: "replacelast", scroll: "end" }
    })
  }

  // If file is empty, we were too fast and will try again on next loop
  if (streamString.length === 0) return JSON.stringify({
    rerun: 0.1,
    variables: { streaming_now: true }
  })

  // If reponse is not finished, continue loop
  if (!finishReason) return JSON.stringify({
    rerun: 0.1,
    variables: { streaming_now: true },
    response: responseText,
    behaviour: { response: "replacelast", scroll: "end" }
  })

  // When finished, write history and delete stream files
  appendAssistantChat(chatFile, reasoningText, contentText)
  deleteFile(streamFile)
  deleteFile(pidStreamFile)

  // Mention finish reason in footer
  const footerText = (function() {
    switch (finishReason) {
      case "length": return "Maximum number of tokens reached"
      case "content_filter": return "Content was omitted due to a flag from content filters"
    }
  })()

  // FUTURE: Add token consumption information

  // Stop
  return JSON.stringify({
    actionoutput: Mode.isPowerfulMode() || Mode.isBotMode(),
    response: responseText,
    footer: footerText,
    behaviour: { response: "replacelast", scroll: "end" }
  })
}

function flushStreamToChatFile(streamFile, chatFile) {
  const streamContent = $.NSString.stringWithContentsOfFileEncodingError(streamFile, $.NSUTF8StringEncoding, undefined).js
  const { reasoningText, contentText, finishReason } = extractContentFromStreamString(streamContent)

  let interruptedContentText = contentText
  if (!finishReason) {
    interruptedContentText = contentText.concat("\n\n[Answer Interrupted]\n\n")
  } else if (finishReason !== "stop") {
    interruptedContentText = contentText.concat(`\n\n[Stop Reason: ${finishReason}]\n\n`)
  }

  appendAssistantChat(chatFile, reasoningText, interruptedContentText)
}

function run(argv) {
  Mode.initFromEnv()
  Provider.initFromEnv()
  TimeoutSeconds.initFromEnv()

  const typedQuery = argv[0]
  const maxContext = parseInt(envVar("max_context"))

  // FUTURE: remove the "ark_" prefix from environment variables, do some compatibility check

  const apiKey = envVar("ark_api_key")

  let apiEndpoint = ""
  if (Provider.isVolcengine()) {
    apiEndpoint = Mode.isBotMode() 
      ? envVar("ark_bot_api_endpoint") || "https://ark.cn-beijing.volces.com/api/v3/bots/chat/completions" 
      : envVar("ark_api_endpoint") || "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
  } else if (Provider.isDeepseek()) {
    apiEndpoint = envVar("deepseek_cc_api_endpoint") || "https://api.deepseek.com/chat/completions"
  } else if (Provider.isOpenrouter()) {
    apiEndpoint = envVar("openrouter_api_endpoint") || "https://openrouter.ai/api/v1/chat/completions"
  }

  // sleect model endpoint according to current mode and provider
  // env name will always be like "ark_xxxx" for compatibility, because no other providers but volcengine at first
  let targetModelEndpoint = ""
  if (Provider.isVolcengine()) {
    targetModelEndpoint = Mode.isBotMode() ? envVar("ark_bot_endpoint") :
      Mode.isPowerfulMode() && envVar("ark_powerful_model_endpoint") ?
        envVar("ark_powerful_model_endpoint") : 
        envVar("ark_model_endpoint")
  } else if (Provider.isDeepseek()) {
    targetModelEndpoint = Mode.isPowerfulMode() && envVar("ark_powerful_model_endpoint") ?
      envVar("ark_powerful_model_endpoint") :
      envVar("ark_model_endpoint")
  } else if (Provider.isOpenrouter()) {
    targetModelEndpoint = Mode.isBotMode() ? envVar("ark_bot_endpoint") :
      Mode.isPowerfulMode() && envVar("ark_powerful_model_endpoint") ?
        envVar("ark_powerful_model_endpoint") :
        envVar("ark_model_endpoint")
  }

  const systemPrompt = envVar("system_prompt")
  
  console.log("alfred_workflow_cache dir:", envVar("alfred_workflow_cache"))
  const chatFile = `${envVar("alfred_workflow_data")}/${AssistantName}_chat.json`
  const pidStreamFile = `${envVar("alfred_workflow_cache")}/${AssistantName}_pid.txt`
  const streamFile = `${envVar("alfred_workflow_cache")}/${AssistantName}_stream.txt`

  const streamingNow = envVar("streaming_now") === "1"
  const interrupted = envVar("interrupted") === "1"

  // if in interrupted branch, then flush streamFile content to chatFile
  if (interrupted) {
    flushStreamToChatFile(streamFile, chatFile)
    return JSON.stringify({
      variables: { streaming_now: false },
      response: markdownChat(readChat(chatFile), false),
      behaviour: { scroll: "end" }
    })
  }

  // if streamingNow is true, then continue streaming
  if (streamingNow) return readStream(streamFile, chatFile, pidStreamFile)

  // if streamingNow is not set, but streamFile exists, then the window was closed in the middle
  if (fileExists(streamFile)) {
    // if user query is empty, continue to display previous streaming content
    if (typedQuery.length === 0) return JSON.stringify({
      rerun: 0.1,
      variables: { streaming_now: true, stream_marker: true },
      response: markdownChat(readChat(chatFile), true),
      behaviour: { scroll: "end" }
    })

    // user query is not empty
    // flush streamFile content to chatFile before proceeding current query
    flushStreamToChatFile(streamFile, chatFile)

    // stop previous streaming and remove files
    forcedCleanup(pidStreamFile, streamFile)
  }

  const previousChat = readChat(chatFile)

  // if the argument is empty, return the previous conversation
  if (typedQuery.length === 0) return JSON.stringify({
    response: markdownChat(previousChat, false),
    behaviour: { scroll: "end" }
  })

  // append new question to chat
  const appendQuery = { role: "user", content: typedQuery, mode: Mode.get() }
  const ongoingChat = previousChat.concat(appendQuery)
  const contextChat = ongoingChat.slice(-maxContext)

  startStream(apiEndpoint, apiKey, targetModelEndpoint, systemPrompt, contextChat, streamFile, pidStreamFile)
  
  appendChat(chatFile, appendQuery)
  
  return JSON.stringify({
    rerun: 0.1,
    variables: { streaming_now: true, stream_marker: true },
    response: markdownChat(ongoingChat)
  })
}